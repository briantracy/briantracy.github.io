<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="description" content="Brian Tracy Brown University Palo Alto personal website"><title>Brian Tracy - bthreads</title><link href="data:," rel="icon"><style>main{margin:auto;max-width:48em}pre{background-color:#d3d3d3;padding:1em}code{background-color:#d3d3d3}footer{font-style:italic;margin-top:3em}hr{width:60%}h1,h2,h3,h4,h5{color:#006400}table{border-collapse:collapse;border-spacing:0}th,td{border:2px solid silver;padding:4px}</style></head><body><main><h1>bthreads &mdash; Bad Threading Library</h1><p>This writeup will walk through how to make an extremely barebones threading library for programs running on Linux, without support from <code>pthreads</code> or the <code>futex</code> syscall.</p><h3>Motivation</h3><p>This year in school, I was exposed to many different "flavors" of systems programming (OS, formal methods of verification, databases, networks, ...). By far the most interesting was the topic of how a uni/multi processor system can support multiple threads of execution. Particularly, the concept of mutual exclusion was intriguing, and I thought it would be a cool project to try to implement my own mutex using commonly available hardware features.</p><p>Of course, a mutex isn't very interesting if you don't have multiple threads of control competing for it, so in order to test the mutexes I was creating, I also needed a way to create threads. This inspired the rest of the "library".</p><p>Code snippets presented in this writeup are distilled versions of the full library. They capture the critical points, but if you want to follow along in full, <a href="https://github.com/briantracy/bthread">you can view the source here</a>.</p><h3>Setup</h3><p>I wanted the following three features in <code>bthreads</code>:</p><ol type="1"><li>The ability to create threads</li><li>The ability to join these threads</li><li>The ability to synchronize these threads via mutexes</li></ol><p>This pretty much dictates the public API of <code>bthreads</code> as follows:</p><pre><code>int  bthread_create(int (*thread_func)(void *), void *thread_arg);
void bthread_collect();

void bthread_mutex_lock(bthread_mutex *mtx);
void bthread_mutex_unlock(bthread_mutex *mtx);</code></pre><p>For the sake of simplicity, joining threads is done all at once in <code>bthread_collect</code>, which should be called by the main thread before it exits.</p><h3>Creating Threads</h3><p>A goal of threads is to have multiple concurrent instruction streams, all with the capability of accessing a shared piece of memory. One method for accomplishing this would be to use <code>fork</code> to create a new process for each thread, then use <code>mmap</code> to map in a shared memory region into each process' address space.</p><p>This approach felt a little clunky as the shared memory regions would have to be explicitly "named" by client programs (i.e: please create a new process, and map <em>these</em> chunks of my address space into the child).</p><p>Instead of manually performing mappings for memory that the threads would like to share, we can ask the kernel to place the threads inside of the parent's address space via the <code>clone</code> system call.</p><p><em>Aside: The concept of "clone" throws a wrench in the traditional distinction between threads and processes students are initially introduced to. For example, the objects created by clone have their own distinct process ids, yet they can all be running within the same address space! One explanation I have read is that the process vs thread distinction is really a user space abstraction, and that the kernel sees both types of objects as simply "tasks". This thread/process duality is used below with respect to creation/joining.</em></p><p>With <code>clone</code>, thread creation distills down to a single call of the following form:</p><pre><code>int child_pid = clone(
    thread_func, (void *)stack_top,
    CLONE_VM | SIGCHLD,
    thread_arg
);</code></pre><p><code>thread_func</code> is the function the user would like to run in the new thread, <code>stack_top</code> points to a region of the address space that was previously allocated for this thread to run inside of, and the flags passed in are as follows:</p><ol type="1"><li><code>CLONE_VM</code>: Tells the kernel to run this thread in the same address space as the one in which it was created. This follows the classical "thread-like" model.</li><li><code>SIGCHLD</code> tells the kernel to ensure that when the newly created thread exits, <code>SIGCHLD</code> should be sent to the parent. This allows the parent to <code>wait</code> upon the child as if we were using the classical "process-like" model.</li></ol><p><code>bthread_create</code> is just a wrapper around <code>clone</code> that creates the stack and passes in the right flags.</p><p>So now we have the capability to fracture one address space into many stacks, have a thread running on each stack, and have all the threads access the same shared memory.</p><p><em>Aside: At this point (no synchronization), our little library is quite dangerous as the client has no way to prevent data races between all that shared memory.</em></p><h3>Joining Threads</h3><p>Before the main thread (the one that spawned all the others via <code>btrhread_create</code>) exits, we would like all of the children to terminate. The common description of this action would be to say that we want to have the parent thread "join with" the child threads (see <code>pthread_join</code>).</p><p>Due to the thread/process duality that is a result of <code>clone</code>, we can leverage the standard <code>wait</code> family of system calls to simulate joining with threads.</p><p>Assuming we have a list of the pids from all of the <code>clone</code> calls (<code>thread_ids</code> below), as well as the total number of threads created (<code>next_tid</code> below), we can repeatedly call <code>wait</code> to collect all of them.</p><pre><code>for (int i = 0; i &lt; next_tid; i++) {
    int status;
    if (waitpid(thread_ids[i], &amp;status, 0) == -1) {
        fprintf(stderr, &quot;... failed to join with thread %d\n&quot;, i);
    }
    fprintf(stderr, &quot;... thread %i exited with value %d\n&quot;, i, WEXITSTATUS(status));
}</code></pre><p><em>Aside: Technically, the call to <code>WEXITSTATUS</code> has no meaning unless the process cleanly exited (as apposed to being killed by a signal), but this is glossed over for simplicity's sake. In fact, most things about this "library" completely ignore best practices in favor of greater readability.</em></p><p>Now that our threads can be spawned, and then reigned in when they are done working, we can move on to synchronizing the threads during their execution.</p><h3>Implementing Mutexes (Overview)</h3><p>To implement a mutex, we need some help from the hardware (<em>Note: this is false in certain situations: <a href="https://en.wikipedia.org/wiki/Peterson%27s_algorithm" class="uri">https://en.wikipedia.org/wiki/Peterson%27s_algorithm</a></em>). x86 provides the <code>lock</code> prefix for instructions, which allows for atomic memory access. This is essential because to properly implement a synchronization object, there needs to be a way to impose an order on the otherwise chaotic nature of multi processor memory access. The following scenario needs to be avoided at all costs:</p><ol type="1"><li>Thread 1 arrives at the beginning of the critical section and sees that the mutex is unlocked.</li><li>Thread 1 claims the mutex, and says to everyone else "this is now locked".</li><li>Before Thread 2 hears the proclamation that "this is now locked", it approaches the critical section and sees that it is open.</li><li>Thread 2 claims the mutex and joins Thread 1 in the critical section.</li></ol><p>The crux of the issue is that the act of noticing that the mutex is unlocked, and then proceeding to lock it (steps 1 and 2 above), is not instantaneous. In the short window between these actions, another thread may also see that the mutex is unlocked and enter the critical section.</p><p>The solution is to combine these two steps into one <strong>atomic</strong> action (or at least a sequence of actions that appear to be atomic).</p><h3>Compare and Exchange (mnemonic <code>cmpxchg</code>)</h3><p>The <code>cmpxchg %reg, dest</code> instruction compares the current value of the accumulator to the value in <code>dest</code> (either another register or a location in memory). If the two are equal, the value in <code>%reg</code> (any general purpose register) is stored into <code>dest</code>. If they are not equal, the value in <code>dest</code> is loaded into the accumulator.</p><p>Note that the value in the accumulator after the execution of a <code>cmpxchg</code> will always be equal to the value that was in <code>dest</code> before the instruction (in one case, they were already equal. In the other, <code>dest</code> is put into the accumulator). In addition, the contents of <code>%reg</code> are never changed.</p><p><em>Aside: The AT&amp;T style syntax is used</em></p><p>Here are two examples showing the two possible execution paths of a compare and exchange (one where the accumulator is equal to the destination, and one where they differ). In both examples, we will use three registers to demonstrate:</p><ol type="1"><li>The "test subject", whose value we are interested in.</li><li>The "test value", a number we want to compare with the test subject.</li><li>The "new value", a number that will replace the test subject if the comparison succeeds.</li></ol><p><strong>Example 1:</strong> Test Value &ne; Test Subject</p><pre><code>mov     $0x1,%eax     ; eax = 1  (test value)
mov     $0x7,%ebx     ; ebx = 7  (new value)
mov     $0x9,%ecx     ; ecx = 9  (test subject)
cmpxchg %ebx,%ecx     ; bang

; *** NEW VALUES ***
; eax = 9  (changed!!)
; ebx = 7  (unchanged)
; ecx = 9  (unchanged)
</code></pre><p><strong>Example 2:</strong> Test Value = Test Subject</p><pre><code>mov    $0x1,%eax      ; eax = 1 (test value)
mov    $0x7,%ebx      ; ebx = 7 (new value)
mov    $0x1,%ecx      ; ecx = 1 (test subject)
cmpxchg %ebx,%ecx     ; bang

; *** NEW VALUES ***
; eax = 1 (unchanged)
; ebx = 7 (unchanged)
; ecx = 7 (changed!!)</code></pre><h3>Lock State</h3><p>If we represent the lock state of a mutex as a number (0 = unlocked, 1 = locked), then the act of checking if a mutex is locked, and then subsequently locking it, can be done via a compare and exchange.</p><pre><code>mov $0, %eax         ; 0 = unlocked (comparing mtx state with this)
mov $1, %ebx         ; 1 = locked   (hopefully set mtx state to this)
                     ; given: %ecx holds the location
                     ; of the mutex state
cmpxchg %ebx, (%ecx)

; %eax now holds the value of the previous mtx state
; so if we compare it with 0 (unlocked), this means that
; the mutex was unlocked, and that we successfully locked it
cmp %eax, $0
je .got_mutex
; if %eax is 1, this means that the mutex was locked
; when we checked it.</code></pre><h3>Implementation in C</h3><p>An easy way to have the compiler emit a <code>cmpxchg</code> instruction is to manually direct it to do so with an in-line assembly block. Assuming that the lock state of our mutex is located in the variable <code>mtx-&gt;locked</code>, the following code will attempt to lock the mutex one time.</p><pre><code>int existing_value = -1;
asm volatile (
    &quot;movl $0, %%eax;&quot;
    &quot;movl $1, %%edx;&quot;
    &quot;lock cmpxchg %%edx, %0;&quot;
    &quot;movl %%eax, %1;&quot;
    : &quot;=m&quot; (mtx-&gt;locked), &quot;=r&quot; (existing_value)
    :
    : &quot;eax&quot;
);
// previous lock state now in existing_value</code></pre><p>This construct currently only tries once for the mutex. This would not be useful in the most general case (<em>for the non general case, see <code>pthread_mutex_trylock</code></em>), but provides the core of what will become a spin lock.</p><p>To force a thread to wait until it acquires the mutex, we can simply wrap the above construct in an infinite loop, halting the threads process until it acquires the mutex.</p><pre><code>void spin_lock(mutex *mtx) {
    while (1) {
        int existing_value;
        &lt;... inline asm from above ...&gt;
        if (existing_value == 0) {
            // got the mutex
            return;
        }
    }
}</code></pre><p>That is roughly the entirety of <code>bthreads</code>: a call to <code>clone</code>, a call to <code>wait</code>, and one <code>cmpxchg</code> instruction.</p><h3>Sample Program</h3><p>Here is a sample program (<code>client.c</code>) that uses all three features of the <code>bthreads</code> library.</p><p><em>Aside: The <code>print</code> function used below is not included for brevity's sake</em></p><pre><code>// Will automatically be initialized to have its
// locks state set to 0 (unlocked).
bthread_mutex mtx;
// This variable will be modified by all threads
int protect_me = 12;
// Total number of times a thread enters the critical
// section. Used to make sure all the threads actually
// get a chance to modify the shared memory.
int critical_section_count = 0;

int thread_fun(void *arg) {
    print(&quot;hello&quot;, (char *)arg);
    for (int i = 0; i &lt; ITERS; i++) {
        bthread_mutex_lock(&amp;mtx);
        critical_section_count++;
        assert(protect_me == 12);
        protect_me++;
        assert(protect_me == 13);
        protect_me--;
        assert(protect_me == 12);
        bthread_mutex_unlock(&amp;mtx);
    }
    print(&quot;goodbye&quot;, (char *)arg);
    exit(0);
}

int main(int argc, char *argv[]) {
    if (argc &lt; 2) {
        fprintf(stderr, &quot;usage: %s [threadname1] ... [threadnameN]\n&quot;, argv[0]);
        return 1;
    }
    const int nthreads = argc - 1;

    printf(&quot;I am the parent, my pid is %d\n&quot;, getpid());

    for (int i = 0; i &lt; nthreads; i++) {
        int s = bthread_create(&amp;thread_fun, argv[i + 1]);
        if (s == -1) {
            fprintf(stderr, &quot;thread creation failed\n&quot;);
            bthread_collect();
            exit(1);
        }
    }
    
    bthread_collect();
    assert(critical_section_count == nthreads * ITERS);
}</code></pre><p>When run, we get the following output.</p><pre><code>$ gcc client.c bthread.c -o client
$ ./client a b c d e f g
I am the parent, my pid is 2991
collecting threads ...
hello from pid 2998, name=g
hello from pid 2994, name=c
hello from pid 2995, name=d
hello from pid 2997, name=f
hello from pid 2996, name=e
hello from pid 2992, name=a
hello from pid 2993, name=b
goodbye from pid 2996, name=e
goodbye from pid 2995, name=d
goodbye from pid 2993, name=b
goodbye from pid 2992, name=a
goodbye from pid 2994, name=c
... thread 0 exited with value 0
... thread 1 exited with value 0
... thread 2 exited with value 0
... thread 3 exited with value 0
... thread 4 exited with value 0
goodbye from pid 2998, name=g
goodbye from pid 2997, name=f
... thread 5 exited with value 0
... thread 6 exited with value 0
... bye</code></pre><p>For some semblance of proof that the mutex is actually working, when the line that locks the mutex is commented out, this is the output.</p><p><em>Aside: I think this is technically undefined behavior (unprotected read/writes to shared memory from multiple threads), see the end for more on that.</em></p><pre><code>I am the parent, my pid is 3024
collecting threads ...
hello from pid 3025, name=a
hello from pid 3026, name=b
hello from pid 3027, name=c
hello from pid 3028, name=d
client: client.c:38: thread_fun: Assertion `protect_me == 12&#39; failed.
hello from pid 3029, name=e
hello from pid 3030, name=f
hello from pid 3031, name=g
Aborted</code></pre><p>This is the end of the informative section of the writeup. I will now move on to the explorative (read: I don't know what's happening, so I can't speak with any confidence) section. If you see anything suspect, please let me know as I want to get to the bottom of this.</p><hr><h3>Measuring Performance</h3><p>What does it mean for a spin lock to be "fast"? When a thread is blocked on a spin lock, it is not making any progress, so it is not clear how one could talk about speed in this context.</p><p>How long a thread waits at the edge of a critical section is a function of what other threads are doing inside, so that is not really up to the implementation of the mutex.</p><p>One way to measure performance of a spin lock could be to see how the overall system is impacted by threads waiting at a spin lock. The only thing the <code>bthread_mutex_lock</code> function does is repeatedly execute a <code>lock cmpxchg</code>, so if a bottleneck exists, it will be here.</p><h3>Lock Prefix and Atomic Memory Access</h3><p>Without a deep knowledge of how the processor implements <code>lock</code>ed instructions, it feels safe to assume that instructions that modify memory atomically are "slower" than those that modify memory non-atomically.</p><p><em>Aside: My weak argument for this claim is that if atomic memory access were just as fast as non-atomic access, it would be the default. In addition, my basic understanding of caching layers tells me that wrangling the caches on a set of processors in such a way that we don't write to the same location simultaneously is a non trivial task.</em></p><p>Additionally, atomically modifying memory might not only be slower for the executing thread, but it might slow down the rest of the threads on the system</p><p><em>Aside: An argument for this claim would be that other threads attempting to access the same memory need to be prevented from doing so until the "atomic thread" has done its work.</em></p><p>So an initial optimization might be to reduce the number of atomic memory accesses a given thread is performing while waiting at a spin lock.</p><p>Keeping in mind that the worst case scenario for a mutex is a false positive (a thread thinks the mutex is open and enters prematurely) and that a false negative scenario (a thread believes the mutex is locked, when in reality, it is unlocked), we can relax the constraints on mutex acquisition logic to favor a false negative.</p><p>If allowing more false negatives (but never relaxing our intolerance of false positives) improves performance, this would be a fair tradeoff. To do this, we can perform normal, non-atomic memory accesses in our spin loop, and only when they say that the mutex is unlocked do we perform an atomic memory access to verify.</p><pre><code>// Un-optimized
while (1) {
    ...
    // perform an atomic cmpxchg every iteration
}

// optimized
while (1) {
    if (mtx-&gt;locked != 0) {
        // regular memory access tells us that the
        // mutex is locked. Instead of atomically
        // verifying this assumption (which is most likely true)
        // just wait a few more cycles.
        continue;
    }
    // regular memory access tells us that the mutex
    // is unlocked! We are hopeful at this point, but
    // need to atomically verify.
    ...
    // perform an atomic cmpxchg
}
</code></pre><p>This reduces the number of atomic operations that are performed, without jeopardizing the strict no-false-positive semantics.</p><h3>Benchmark</h3><p>To see if this optimization makes a difference, I cooked up a benchmark where while in the critical section, each thread performs a huge number of reads and writes. The memory operations are structured such that the order is completely random (to reduce predictive caching) and the search space (<code>ARRAY_SIZE</code>) is large enough to prevent locality based caching.</p><p>The goal here is to "slam the memory bus" to aggravate any adverse side effects that performing a <code>lock</code>ed instruction might have on the rest of the system.</p><pre><code>int massive_array[ARRAY_SIZE];
static unsigned long total = 0;
void slam_bus() {
    // Perform lots of writes and reads to memory
    // Try to avoid hitting caches.
    int idx = rand() % ARRAY_SIZE;
    for (int i = 0; i &lt; BUS_SLAM_ROUNDS; i++) {
        int next_idx = massive_array[idx];
        // next thread that comes here will go somewhere totally different
        massive_array[idx] = (ARRAY_SIZE - 1) - massive_array[idx];
        idx = next_idx;
        total += idx;
    }
}

bthread_mutex mtx;
int protect_me = 12;

int thread_fun(void *arg) {
    (void)arg;
    for (int i = 0; i &lt; THREAD_FUNC_REPS; i++) {
        bthread_mutex_lock(&amp;mtx);
        assert(protect_me == 12);
        protect_me++;
        assert(protect_me == 13);
        protect_me--;
        assert(protect_me == 12);

        slam_bus();

        bthread_mutex_unlock(&amp;mtx);
    }
    exit(0);
}</code></pre><p>To see the results, I compile one benchmark with the optimization and one without.</p><pre><code>$ diff bthread-with-opt.c bthread-without-opt.c 
141c141
&lt;         if (mtx-&gt;locked != 0) { continue; }
---
&gt;         //if (mtx-&gt;locked != 0) { continue; }

$ gcc benchmark.c bthread-with-opt.c &amp;&amp; time ./a.out
            &lt;output trimmed&gt;
... bye
total = 1638299986875

real    0m17.612s
user    0m35.155s
sys 0m0.021s

$ gcc benchmark.c bthread-without-opt.c &amp;&amp; time ./a.out
            &lt;output trimmed&gt;
... bye
total = 1638299986875

real    0m30.714s
user    1m1.370s
sys 0m0.025s</code></pre><p>So the optimized version is definitely faster. How much faster is a function of what we are doing in the critical section, and how many threads are spinning in wait.</p><p>This is the end of the explorative section. Moving forward, I will be discussing the things that I can't make any sense out of.</p><hr><h3>Optimization</h3><p>All of the examples shown have been compiled without optimizations. Applying even <code>-O1</code> breaks the programs, but not because of an issue with the mutex functions (the optimizations <code>gcc</code> makes are really readable. It almost looks hand written):</p><pre><code>$ gcc -c -O1 bthread.c &amp;&amp; objdump -d bthread.o | less
0000000000000218 &lt;bthread_mutex_lock&gt;:
 218:   8b 07           mov    (%rdi),%eax
 21a:   85 c0           test   %eax,%eax
 21c:   75 fc           jne    21a &lt;bthread_mutex_lock+0x2&gt;
 21e:   b8 00 00 00 00  mov    $0x0,%eax
 223:   ba 01 00 00 00  mov    $0x1,%edx
 228:   f0 0f b1 17     lock cmpxchg %edx,(%rdi)
 22c:   89 c2           mov    %eax,%edx
 22e:   85 d2           test   %edx,%edx
 230:   75 e6           jne    218 &lt;bthread_mutex_lock&gt;
 232:   f3 c3           repz retq </code></pre><p>Both the client program and the benchmark invariable hang while waiting for one of the threads to exit. I don't know why, but optimization does not seem to agree with my amalgamation of clone and wait.</p><p><em>Aside: I am almost positive that this is not an issue with clone and wait. These are extremely well tested functions that were written by RealProgrammers&TRADE;. The issue is probably some undefined behavior I am invoking in the way <code>bthreads</code> is setup.</em></p><h3>Conclusion</h3><p>This writeup was made in an attempt to codify what I think I learned while digging deeper into multi threaded programming on Linux. Please feel free to reach out and correct my mistakes. Thanks for reading.</p><p>Here are some cool resources.</p><p><a href="https://www.felixcloutier.com/x86/cmpxchg" class="uri">https://www.felixcloutier.com/x86/cmpxchg</a></p><p><a href="https://eli.thegreenplace.net/2018/launching-linux-threads-and-processes-with-clone/" class="uri">https://eli.thegreenplace.net/2018/launching-linux-threads-and-processes-with-clone/</a></p></main><footer>Last modified May 27 12:03:59 2020</footer></body></html>